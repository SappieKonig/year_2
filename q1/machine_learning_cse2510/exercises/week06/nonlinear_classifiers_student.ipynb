{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6: Non-linear Classifiers\n",
    "Machine Learning 2021/2022 <br>\n",
    "Ruben Wiersma and Odette Scharenborg\n",
    "\n",
    "Revised by Lisette Veldkamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WHAT** This nonmandatory lab consists of several programming and insight exercises/questions on decision trees.\n",
    "\n",
    "**WHY** The exercises are meant to familiarize yourself with the basic concepts of non-linear classifiers, in particular decision trees.\n",
    "\n",
    "**HOW** Follow the exercises in this notebook either on your own or with a friend. If you want to skip right to questions and exercises, find the $\\rightarrow$ symbol. For questions and feedback please consult the TAs during the lab session. \n",
    "\n",
    "$\\newcommand{\\q}[1]{\\rightarrow \\textbf{Question #1}}$\n",
    "$\\newcommand{\\ex}[1]{\\rightarrow \\textbf{Exercise #1}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "In Lab 4, you have practiced with linear classifiers, specifically with logistic regression. In this assignment, you will work on a non-linear classfier: you will build an implementation of a univariate decision tree algorithm that can classify data based on both discrete and numeric variables. The exercise consists of the following parts:\n",
    "1. Introduction: Introduction to problems which require a non-linear decision boundary.\n",
    "2. The Dataset: Understand the data to be classified.\n",
    "3. Entropy and Information Gain: Measure the quality of a split in a decision tree.\n",
    "4. Creating Decision Trees: Finish the implementation for discrete trees.\n",
    "5. Adding the Numeric Tree: Finish the implementation for numeric trees.\n",
    "6. Comparing Results: Compare validation results among a discrete, numeric and hybrid tree.\n",
    "7. Analysis: Evaluate the final results.\n",
    "\n",
    "Let's get to it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Decision trees are non-linear classifiers. In other words: we can separate data with a decision boundary that does not resemble a single line. Why would this be useful? \n",
    "\n",
    "Think of the XOR problem given by the following points:\n",
    "\n",
    "- $X = (0, 1); (1, 0);$ \n",
    "\n",
    "- $O = (0, 0); (1, 1);$\n",
    "\n",
    "```    \n",
    "0    o   x\n",
    "  \n",
    "1    x   o\n",
    "\n",
    "     0   1\n",
    "\n",
    "```\n",
    "<br>\n",
    "\n",
    "$\\ex{1.1}$ Try to draw a line in the drawing above that can separate the `x`s from the `o`s.\n",
    "\n",
    "Quite impossible, right? Now let's try this with a decision tree: we set up a tree where each node represents a decision and the children of a node represent the choices for this decision. An example of such a tree for the above problem could be:\n",
    "\n",
    "```\n",
    "          x < 0.5?\n",
    "           /   \\\n",
    "         yes   no\n",
    "         /       \\\n",
    "    y < 0.5?    y < 0.5?\n",
    "      /  \\       /  \\\n",
    "    yes  no    yes   no\n",
    "    /     \\     /     \\\n",
    "   o      x    x       o\n",
    "```\n",
    "<br>\n",
    "\n",
    "$\\ex{1.2}$ Follow the decision tree for each point in the x-or problem and verify that the points are classified correctly.\n",
    "\n",
    "You will implement the code to build such a decision tree in the rest of this assignment. We will return to this example at the end of this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Dataset: Heart Disease \n",
    "\n",
    "We will use decision trees to predict whether a patient has a heart disease using a dataset containing symptoms, prescriptions, and diagnoses from four different hospitals. The dataset can be found [here](https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/). A description of the dataset is provided in the file named [heart-disease.names.txt](data/heart-disease.names.txt). \n",
    "\n",
    "Look through the file and take a good look at sections 4 and 7. Section 4 lists some relevant information concerning the dataset and section 7 describes the different variables that are in the dataset. The dataset contains both discrete and continuous variables:\n",
    "- An example of a discrete variable is attribute #9, *chest pain type*, with four different possible labels for the chest pain type.\n",
    "- An example of a continuous variable is attribute #12, *serum cholestoral in mg/dl*, with the concentration of cholesterol in mg/dl.\n",
    "\n",
    "Decision trees are particularly good at handling both discrete and continuous variables, so they could be a good classifier for this dataset.\n",
    "\n",
    "### Understanding the dataset\n",
    "\n",
    "The heart disease directory contains four datasets from different hospitals. We have created a cleaned-up version of the dataset where patient records from all four hospitals are aggregated together. You can load it in using NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.load('data/heart_disease.npy')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us 299 patient records, most of which are from the Cleveland hospital. For each patient we have 14 features, but we do not know which ones are discrete and which ones are continuous yet.\n",
    "\n",
    "Instead of studying the description of each feature, we will try to find out by counting the number of unique values for each of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_data = np.sort(data, axis=0)\n",
    "frequencies = (sorted_data[1:,:] != sorted_data[:-1,:]).sum(axis=0) + 1\n",
    "frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\q{2.1}$ Which features are likely to be discrete and which numeric?\n",
    "\n",
    "We also see that the last column, the actual diagnosis, has 5 possible values. These are the labels.\n",
    "\n",
    "Let's visualise how these labels are distributed using a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(data[:, 13], np.arange(0, 4 + 1.5) - 0.5)\n",
    "plt.title('The distribution of labels in the dataset')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Label')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\q{2.2}$ What stands out in this histogram? What does this mean in terms of diagnosis?\n",
    "\n",
    "We will create a univariate decision tree to decide if an entry belongs to one specific diagnosis or not (binary decision). While decision trees can handle multi-class problems, we will build one to decide for each entry whether it belongs to class $0$ (absence of heart disease) or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset\n",
    "\n",
    "In order to train and validate the decision trees, we split the dataset into a training and validation set and separate each of these into three arrays:\n",
    "\n",
    "1. `x_discrete`, a 2d-array of integers containing the discrete variables for each patient.\n",
    "2. `x_numeric`, a 2d-array of floats containing the numeric variables for each patient.\n",
    "3. `y`, a 1d-array of booleans indicating for each patient whether the diagnosis was class 0 or not. This array contains the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the array into features and labels\n",
    "x = data[:, :13]\n",
    "y = data[:, 13]\n",
    "\n",
    "# Transform classes to booleans\n",
    "# y = (y == 0), Numpy will repeat this equality check for each entry in the array\n",
    "# and return an array of booleans.\n",
    "y = y == np.zeros(len(y))\n",
    "\n",
    "def split_dataset(x, y, random_state):\n",
    "    # Split data into train and validation\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # For this assignment, we state the random_state variable.\n",
    "    # This variable will be used as the seed for the random number generation so that the split is deterministic.\n",
    "    # Therefore, all exercises will give the same results every run.\n",
    "    x_train, x_validation, y_train, y_validation = train_test_split(x, y, test_size=0.3, random_state=random_state) \n",
    "\n",
    "    # Separate features into discrete and numeric arrays. \n",
    "    # You can verify that the split (with a boundary of 5) is correct by looking at the data documentation.\n",
    "    x_train_discrete = x_train[:, np.where(frequencies < 5)[0]].astype(int)\n",
    "    x_train_numeric = x_train[:, np.where(frequencies > 5)[0]]\n",
    "    x_validation_discrete = x_validation[:, np.where(frequencies < 5)[0]].astype(int)\n",
    "    x_validation_numeric = x_validation[:, np.where(frequencies > 5)[0]]\n",
    "    \n",
    "    return x_train_discrete, x_train_numeric, x_validation_discrete, x_validation_numeric, y_train, y_validation\n",
    "\n",
    "x_train_disc, x_train_num, x_validation_disc, x_validation_num, y_train, y_validation = split_dataset(x, y, 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ex{2.1}$ Print the six arrays and check that the shapes are correct. Verify (with Python assertions) that:\n",
    "1. `y_train` has the same number of rows as `x_train_disc` and `x_train_num`  \n",
    "2. `y_validation` has the same number of rows as `x_validation_disc` and `x_validation_num`\n",
    "3.  there are 8 discrete variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START ANSWER\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Entropy and Information Gain\n",
    "\n",
    "A decision tree splits a dataset based on the values of certain features. To find the best features and values to split on, we need some way to measure the quality of a split. We will use information gain (which is based on the entropy in the nodes) for this purpose.\n",
    "\n",
    "<img src=\"images/entropy-split.png\" alt=\"Entropy Split\" style=\"height: 350px;\"/>\n",
    "\n",
    "*Image retrieved from Ricaud, B. (2017, August 27). A simple explanation of entropy in decision trees.  https://bricaud.github.io/personal-blog/entropy-in-decision-trees*\n",
    "\n",
    "### Entropy\n",
    "\n",
    "For decision trees we use the information theoretic entropy, also known as Shannon entropy. It tells us about the amount of information contained in a certain distribution of data.\n",
    "The best split is the split on a feature that separates most of the \"1s\" from the \"0s\" in the resulting two sets. Entropy can thus also be regarded as a measure of purity, and we aim to increase the purity of nodes.\n",
    "\n",
    "An entropy close to 1.0 indicates that a subset of the data contains an equal number of labels \"1\" and \"0\", and thus a split resulting in such subsets is not useful. In the graph below, the relation between entropy and the proportion of data points belonging to one class (in this case '+') in a data set is plotted. As can be seen in the image, the entropy is maximal when the set contains an equal number of \"1\" and \"0\" labels. At this point the uncertainty is the highest. The entropy decreases as the data set becomes 'purer'. Our goal is to decrease the entropy by making proper splits.\n",
    "\n",
    "\n",
    "<img src=\"images/entropy.png\" alt=\"Entropy\" style=\"height: 350px;\"/>\n",
    "\n",
    "*Image retrieved from Tandon, S. (2019, January 11). Entropy: How Decision Trees Make Decisions. https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Shannon entropy for any number of classes is given as:\n",
    "\n",
    "(3.1.a) $$\\phi(p) = −\\sum_i p_i\\ log_2(p_i)$$\n",
    "\n",
    "As mentioned, we will only decide whether an entry belongs to class $0$ or not: True or False. Thus, in or our case, we can re-write Shannon entropy as follows:\n",
    "\n",
    "(3.1.b) $$\\phi(p) = −p\\ log_2(p) − (1 − p)\\ log_2(1 − p)$$\n",
    "\n",
    "where $p$, the probability that an item has label 0, is equivalent to the ratio between the number of items with label $0$ (True) and the number of items with another label (False).\n",
    "\n",
    "\n",
    "$\\ex{3.1}$ First complete the `ratio()` function to compute $p$. The function, given a list of boolean values as class labels, should return the ratio of `True` labels in the list, e.g. $1.0$ would indicate the list only contains `True` labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def ratio(labels):\n",
    "    if len(labels) == 0:\n",
    "        return 0\n",
    "    # START ANSWER\n",
    "    # END ANSWER\n",
    "    \n",
    "print('Ratio for train set:', ratio(y_train))\n",
    "print('Ratio for validation set:', ratio(y_validation))\n",
    "\n",
    "# Verify the correctness of the ratio function\n",
    "assert np.isclose(ratio(y_train), 0.53110)    \n",
    "assert np.isclose(ratio(y_validation), 0.54444)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the entropy. Before we start writing the code, we deal with a possible source of error: the computation of $0\\ log_2(0)$ (when $p$ is $0$) will correctly result in a math error.\n",
    "\n",
    "$\\ex{3.2}$ Complete the function `entropy_sub()` to compute the value of the log product. Make sure to return $0$ in the case that $p$ is $0$ instead of an \n",
    "error. Then combine `ratio()` and `entropy_sub()` to compute the `entropy()` of a list of boolean class labels.\n",
    "\n",
    "**Hint:** Use Python's built in `math.log2()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_sub(p):\n",
    "    \"\"\"\n",
    "    Returns the value for p * log_2(p)\n",
    "    \"\"\"\n",
    "    # START ANSWER\n",
    "    # END ANSWER \n",
    "    \n",
    "def entropy(labels):\n",
    "    \"\"\"\n",
    "    Returns the entropy of an array of labels, computed using equation (3.1.b)\n",
    "    \"\"\"\n",
    "    # START ANSWER\n",
    "    # END ANSWER \n",
    "\n",
    "print('Entropy for train set:', entropy(y_train))\n",
    "print('Entropy for validation set:', entropy(y_validation))\n",
    "\n",
    "# Verify the correctness of the entropy function\n",
    "assert np.isclose(entropy(y_train), 0.9972)    \n",
    "assert np.isclose(entropy(y_validation), 0.9943)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entropies of $s$ sets of labels can be combined using a weighted sum:\n",
    "\n",
    "(3.2) $$I_m = \\sum_{j=1}^s \\frac{N_j}{N} \\phi(p_j)$$\n",
    "\n",
    "This will give us the overall entropy $I$ of a split on variable $m$, where $N$ is the size of the set before the split, $N_j$ is the size of the $j^{th}$ set after the split, and $\\phi(p_j)$ is the entropy of the $j^{th}$ set.\n",
    "\n",
    "$\\ex{3.3}$ Complete the function `split_entropy()` to compute this value for a list of labels and `N`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_entropy(label_lists, N):\n",
    "    information = 0\n",
    "    for label_list in label_lists:\n",
    "        # START ANSWER\n",
    "        # END ANSWER\n",
    "    return information\n",
    "\n",
    "# Verify the correctness of the split_entropy function\n",
    "labels = np.array([0, 0, 0, 0, 1, 1, 1, 1])\n",
    "N = len(labels)\n",
    "print('Entropy of the data before splitting:', entropy(labels))\n",
    "\n",
    "# Worst case split\n",
    "labels_list = np.array([[0, 0, 1, 1], [0, 0, 1, 1]])\n",
    "print('Worst case:', split_entropy(labels_list, N))\n",
    "assert np.isclose(split_entropy(labels_list, N), 1.0)    \n",
    "\n",
    "# Better split\n",
    "labels_list = np.array([[0, 0, 0, 1], [1, 1, 1, 0]])\n",
    "print('Better:', split_entropy(labels_list, N))\n",
    "assert np.isclose(split_entropy(labels_list, N), 0.81128)    \n",
    "\n",
    "# Perfect split\n",
    "labels_list = np.array([[0, 0, 0, 0], [1, 1, 1, 1]])\n",
    "print('Optimal:', split_entropy(labels_list, N))\n",
    "assert np.isclose(split_entropy(labels_list, N), 0.0)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain\n",
    "\n",
    "Information Gain (IG) measures how much the entropy changes by making a specific split, i.e. the relative gain in predictability of the data by creating a specific distribution of labels. IG is defined as the entropy of the original distribution $\\phi(p)$ minus the entropy of the split distribution $I_m$ resulting from the split on variable $m$.\n",
    "\n",
    "$$IG_m = \\phi(p) - I_m$$\n",
    "\n",
    "The IG thus depends on two things: \n",
    "- the previous list of labels;\n",
    "- how these labels are divided into new distributions by the split.\n",
    "\n",
    "$\\ex{3.4}$ Complete the `information_gain()` function using the earlier created functions `entropy()` and `split_entropy()`, with as input the previous list of labels (without a split) and a list of indices for each part of the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(labels, indices): \n",
    "    labels_list = []\n",
    "    for index_list in indices:\n",
    "        labels_list.append(labels[index_list])\n",
    "    # START ANSWER\n",
    "    # END ANSWER\n",
    "    \n",
    "labels = np.array([0, 0, 0, 0, 1, 1, 1, 1])\n",
    "labels_list = np.array([[0, 0, 0, 1], [1, 1, 1, 0]])\n",
    "\n",
    "# Now we create `indices` that correspond to the indices of the split (compare them with the two lists above). \n",
    "# You will have to write code that creates such a list later.\n",
    "indices = np.array([[0, 1, 2, 4], [5, 6, 7, 3]])\n",
    "\n",
    "# Verify the correctness of the information_gain function\n",
    "assert np.isclose(information_gain(labels, indices), 0.18872)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating Decision Trees\n",
    "\n",
    "With the data turned into a usable format and the functions to measure entropy and IG ready, we can start building the actual decision tree. Classes are a great way to represent trees: each DecisionTree object class represents a node (or subtree) in the tree and we only have to store references to that node's children to build the structure of the tree. We will distinguish two types of 'nodes':\n",
    "\n",
    "1. DiscreteTree nodes, split on the basis of the value a discrete variable\n",
    "2. NumericTree nodes, split on the basis of the value of a numeric variable\n",
    "\n",
    "Because these two classes are very similar, we will use *inheritance* to avoid redundancy. First we define a parent class `DecisionTree` that will contain all the common elements for `DiscreteTree` and `NumericTree`.\n",
    "\n",
    "We create a general `DecisionTree` node and then turn it into a `DiscreteTree` or `NumericTree` instance based on the best possible split. This will greatly simplify building a tree which can handle both numeric and discrete splits. The `DecisionTree` class holds all the common functions used for both the `DiscreteTree` and `NumericTree` nodes. Additionally, we will also use its `__init__` method as the generic constructor for either specific type of node. The code to pick the correct concrete class has already been provided, as well as most other logic, so you will only need to focus on completing the split functions for `DiscreteTree` and `NumericTree`.\n",
    "\n",
    "$\\ex{4.1}$ Start by reading the code that has been provided here, so you get a sense of the general structure, and most importantly, what the attributes for each `DecisionTree` nodes are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class DecisionTree(object):\n",
    "    def __init__(self, data_discrete, data_numeric, labels, tree_type=0, thres=0.1):\n",
    "        \"\"\" Creates a Decision Tree, based on the following arguments:\n",
    "                data_discrete - A 2D array of ints, each row containing the discrete features for a patient.\n",
    "                data_numeric - A 2D array of floats, each row containing the numeric features for a patient.\n",
    "                labels - An array of boolean class labels, each corresponding to a\n",
    "                        DataRow instance of a patient at the same index. \n",
    "                tree_type - 0: create the Tree with the highest IG every node \n",
    "                            1: create DiscreteTrees only\n",
    "                            2: create NumericTrees only\n",
    "                thres - The cutoff value for IG, to stop splitting the tree.\n",
    "                        Below this value the node becomes a leaf node and no\n",
    "                        further splits are made.\n",
    "            N.B. This function has already been provided and does not need to be modified.\"\"\"\n",
    "        # Store the basic attributes for any DecisionTree\n",
    "        self.data_discrete = data_discrete\n",
    "        self.data_numeric = data_numeric\n",
    "        self.labels = labels\n",
    "        self.tree_type = tree_type\n",
    "        self.thres = thres\n",
    "        \n",
    "        # Compute the current ratio of labels and assign this node the most common label\n",
    "        self.ratio = ratio(self.labels)\n",
    "        # This will assign a boolean value to self.label, as `self.ratio >= 0.5` is a boolean statement\n",
    "        self.label = self.ratio >= 0.5\n",
    "        \n",
    "        if self.tree_type == 1:\n",
    "            # Convert this DecisionTree to a DiscreteTree and perform the split\n",
    "            discr_tree = DiscreteTree(self)\n",
    "            self.convert_tree(discr_tree)\n",
    "        elif self.tree_type == 2:\n",
    "            # Convert this DecisionTree to a NumericTree and perform the split\n",
    "            numer_tree = NumericTree(self)\n",
    "            self.convert_tree(numer_tree)\n",
    "        else:\n",
    "            # If no specific type has been given (tree_type: 0), we determine which type is best\n",
    "            # by computing both options and comparing the IG.\n",
    "            # Create a DiscreteTree and NumericTree, passing all the stored attributes\n",
    "            # as an argument, and compute the best possible split for each\n",
    "            discr_tree = DiscreteTree(self)\n",
    "            numer_tree = NumericTree(self)\n",
    "            \n",
    "            # Based on the results of the split computations, replace this generic\n",
    "            # DecisionTree node with either a DiscreteTree or a NumericTree node\n",
    "            if discr_tree.info_gain > numer_tree.info_gain:\n",
    "                self.convert_tree(discr_tree)\n",
    "            else:\n",
    "                self.convert_tree(numer_tree)\n",
    "        \n",
    "        # Create an empty dictionary to contain the (possible) branches from this node,\n",
    "        # where the values should be new DecisionTree nodes, or None if not present\n",
    "        self.branches = defaultdict(lambda: None)\n",
    "        \n",
    "        # Check if this split produced a high enough IG to actually create\n",
    "        # the resulting branches with new split nodes below it,\n",
    "        # else no split is carried out and the original node is a leaf node\n",
    "        self.leaf = self.info_gain < self.thres\n",
    "        if not self.leaf:\n",
    "            self.create_subtrees()\n",
    "    \n",
    "    def store_split_values(self, feat_index, feat_values, indices, info_gain):\n",
    "        \"\"\" Stores the values of the passed parameters as object attributes. Is intended\n",
    "            to store the results of a split computation for either a DiscreteTree or a\n",
    "            NumericTree. The stored attributes are:\n",
    "                feat_index - The index of the feature on which the split was\n",
    "                    based.\n",
    "                feat_values - A list of the possible values that this split feature can\n",
    "                    take, each corresponding to a different branch in the DecisionTree\n",
    "                indices - A list of index lists, with each list containing the indices\n",
    "                    defining a subset of the current data and label attributes, as\n",
    "                    computed by the split. The order of these subsets should match the\n",
    "                    order of the corresponding feat_values used to define the branches\n",
    "                    of the split.\n",
    "                info_gain - IG computed for this split\n",
    "            N.B. This function has already been provided and does not need to be modified.\"\"\"\n",
    "        self.feat_index = feat_index\n",
    "        self.feat_values = feat_values\n",
    "        self.indices = indices\n",
    "        self.info_gain = info_gain\n",
    "    \n",
    "    def convert_tree(self, new_tree):\n",
    "        \"\"\" Converts this object to the tree passed as the new_tree parameter.\n",
    "            All attributes from the new_tree are transfered.\n",
    "                new_tree - Either a DiscreteTree or a NumericTree instance, to which\n",
    "                            this object is converted\n",
    "            N.B. This function has already been provided and does not need to be modified.\"\"\"\n",
    "        self.__class__ = new_tree.__class__\n",
    "        self.__dict__ = new_tree.__dict__\n",
    "    \n",
    "    def create_subtrees(self):\n",
    "        \"\"\" Creates the different subsets of the current data and labels, and makes a\n",
    "            a new DecisionTree node for each such subset, based on the indices attribute\n",
    "            stored after the computed split. These new DecisionTrees are stored in the \n",
    "            branches attribute, a dictionary mapping the value of a variable from the\n",
    "            split to the new DecisionTree created by selecting that value for the split.\"\"\"\n",
    "        for i, key in enumerate(self.feat_values):\n",
    "            subset_discrete = self.data_discrete[self.indices[i]]\n",
    "            subset_numeric = self.data_numeric[self.indices[i]]\n",
    "            subset_labels = self.labels[self.indices[i]]\n",
    "            subtree = DecisionTree(subset_discrete, subset_numeric, subset_labels, tree_type = self.tree_type)\n",
    "            self.branches[key] = subtree\n",
    "        \n",
    "    def classify(self, row_discrete, row_numeric):\n",
    "        \"\"\" Traverses the DecisionTree based on the values stored in the given row and\n",
    "            returns the most common label in the resulting leaf node.\n",
    "                row - The index of the row being classified\"\"\"\n",
    "        # Option 1: node is a leaf\n",
    "        if self.leaf:\n",
    "            return self.label\n",
    "        \n",
    "        subtree = self.get_subtree(row_discrete, row_numeric)\n",
    "        \n",
    "        # Option 2: no valid subtree\n",
    "        if subtree is None:\n",
    "            return self.label\n",
    "        \n",
    "        # Option 3: there is a valid subtree\n",
    "        return subtree.classify(row_discrete, row_numeric)\n",
    "        \n",
    "    def split(self):\n",
    "        \"\"\" Must be implemented by the subclass based on the specific type of split performed.\n",
    "            The function here is only to ensure it is implemented, and should not be modified.\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def get_subtree(self, instance):\n",
    "        \"\"\" Must be implemented by the subclass based on the specific type of split performed.\n",
    "            The function here is only to ensure it is implemented, and should not be modified.\"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete split\n",
    "\n",
    "Now you can start actually implementing the `split()` function for the `DiscreteTree`. This function should, for every discrete variable in the data, try to create a split based on that variable and compute the *Information Gain* of the resulting split.\n",
    "\n",
    "For discrete splits, we split the set into subsets: one for each discrete label. For example, if there are three possible labels for a certain feature, we should return three subsets. The question is: which feature should we pick to split on? You can find out by performing the following steps:\n",
    "1. For each feature:\n",
    "    1. Split the set into subsets corresponding to each discrete label.\n",
    "    2. Compute the information gain for this split.\n",
    "2. Split the dataset based on the feature with the highest information gain.\n",
    "\n",
    "Once the best feature for the split has been determined, the results of the split need to be stored in the instance, so they can be used to build the rest of the tree. Let's assume the algorithm decides to split on `chest pain type` (#9). The following attributes should then be stored when splitting:\n",
    "\n",
    "1. The index of the feature to split on (e.g. 11)\n",
    "2. A list of discrete options for this feature (e.g. [0, 1, 2, 3], indicating the type of chest pain)\n",
    "3. A list of indices per option, so the first sublist contains the indices of all rows with chest pain type = 0, etc. (e.g. [[0, 3, 4, 5, 7, 9, ...], [8, ...], [2, 6, ...], [1, ...]])\n",
    "4. The information gain resulting from the split (e.g. 0.8)\n",
    "\n",
    "These attributes can then be used to build the rest of the tree.\n",
    "\n",
    "$\\ex{4.2}$ Write the `create_indices_list()` function following the above steps.\n",
    "\n",
    "__Hint__: You can use `np.unique()` to get the unique values in a list.\n",
    "\n",
    "$\\ex{4.3}$ Complete the `split()` function following the above steps.\n",
    "\n",
    "__Hint__: Remember to reference attributes of a class using `self`, e.g. `self.data_discrete` instead of `data_discrete`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indices_list(column):\n",
    "    \"\"\" Creates the indices list, containing for each possible value of the current feature, \n",
    "        the indices of corresponding rows (e.g. [[0, 2], [1, 3], ...] where the current\n",
    "        feature is 0 in rows 0 and 2).\n",
    "        Returns the list of indices for all feature values and as second output a list of all possible feature values.\n",
    "            column - The column of one feature from the data.\"\"\"\n",
    "    # START ANSWER\n",
    "    # END ANSWER\n",
    "    \n",
    "vals = np.array([0, 0, 1, 0, 1, 1, 2, 2])\n",
    "indices_list, feat_values = create_indices_list(vals)\n",
    "\n",
    "# Verify the correctness of the create_indices_list function\n",
    "assert ([[*i] for i in indices_list] == [[0, 1, 3], [2, 4, 5], [6, 7]])\n",
    "assert np.array_equal(feat_values, [0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteTree(DecisionTree):\n",
    "    def __init__(self, dtree):\n",
    "        \"\"\" Takes a DecisionTree as initialization parameter and copies all its\n",
    "            attributes. Then calls the split() function to determine the optimal\n",
    "            discrete variable to split this subset of the data on.\n",
    "                dtree - The DecisionTree instance whose attributes are copied to this\n",
    "                        DiscreteTree instance.\n",
    "            N.B. This function has already been provided and does not need to be\n",
    "            modified.\"\"\"\n",
    "        self.__dict__ = dtree.__dict__.copy()\n",
    "        self.split()\n",
    "\n",
    "    def split(self):\n",
    "        \"\"\" Determines the best discrete variable to split the current dataset on,\n",
    "            based on the IG resulting from the split. For this best split variable, the\n",
    "            function stores several resulting attributes from the split, using the\n",
    "            store_split_values function. See the documentation of store_split_values\n",
    "            for an overview of what should be stored.\"\"\"\n",
    "        max_feat = None\n",
    "        max_feat_values = None\n",
    "        max_split = None\n",
    "        max_ig = 0\n",
    "        \n",
    "        for feat in range(self.data_discrete.shape[1]):\n",
    "            # 1. Call create_indices_list() for the feature column.\n",
    "            # 2. Compute the IG of the split\n",
    "            # 3. If IG > max IG, update max values\n",
    "            \n",
    "            # START ANSWER\n",
    "            # END ANSWER\n",
    "            \n",
    "        self.store_split_values(max_feat, max_feat_values, max_split, max_ig)\n",
    "            \n",
    "    def get_subtree(self, row_discrete, row_numeric):\n",
    "        \"\"\" Returns the subtree one branch down.\n",
    "            Returns None if the value was not present at the split.\n",
    "                row_discrete - array of the discrete values\n",
    "                row_numeric - array of the numeric values\"\"\"\n",
    "        value = row_discrete[self.feat_index]\n",
    "        return self.branches.get(value, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating subtrees\n",
    "\n",
    "If we were to repeat this process of splitting each node, we end up with a tree structure. But we need to stop at a certain moment to avoid building infinite trees and to avoid overfitting.\n",
    "There are quite a few strategies to decide when to stop. The simplest of these is just to stop splitting when the *Information Gain* of a split drops below a certain threshold.\n",
    "\n",
    "This is already implemented as the last step in the `__init__` function of `DecisionTree`. The computed IG is compared to the threshold and if the gain is too low, the node is labeled as a leaf node. If the node is not a leaf node, then further splits should be attemped and the `create_subtrees()` function is called to populate the branches of this node with new subset `DecisionTrees`.\n",
    "\n",
    "$\\ex{4.4}$ Take out a pen and paper and try to draw the structure that would be built if you created a new `DecisionTree` using the training data. No need to do the actual entropy math on paper, just try and sketch what objects would be created and how they would relate to each other. If you are having trouble visualizing this for such a large dataset, take a look at the small example tree below.\n",
    "\n",
    "<img src=\"images/decision-tree.png\" alt=\"Decision Tree\" style=\"height: 300px;\"/>\n",
    "\n",
    "*Image retrieved from Alpaydin, E. (2010). Introduction to machine learning. Cambridge, Mass.: Mit Press.*\n",
    "\n",
    "$\\ex{4.5}$ Take a look at the `create_subtrees()` function in the `DecisionTree` class above and try to understand how it works.\n",
    "\n",
    "__Note:__ In order to traverse the decision tree, each new `DecisionTree` should be stored in the `branches` dictionary of the node. Here the *key* should be the value of the discrete variable for that split (stored in `self.feat_values`) and the *value* should be the new `DecisionTree` resulting from that choice in the split. When this dictionary is complete, each key-value pair will represent a different branch of the tree at that split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving subtrees and classifying patient records\n",
    "\n",
    "The tree building part of the algorithm is complete now! With this structure built, classifying a new patient record is pretty easy. In the code, we refer to a patient record as a row. The patient record is split into a discrete and numeric part: `row_discrete` and `row_numeric`.\n",
    "\n",
    "$\\ex{4.6}$ Take a look at the `get_subtree()` function in `DiscreteTree`, which should return the `DecisionTree` that corresponds to the value of `row_discrete` at the split feature.\n",
    "\n",
    "The previous exercise gives us a way to traverse the tree, given a patient record. Now, we want to classify the patient record by traversing the tree. At each DecisionTree node, we have three options, based on the type of node:\n",
    "\n",
    "1. The node is a leaf node, in which case the classification will be the most common label of that node\n",
    "2. The node does not have a valid subtree for the splitted value, so the classification will also be the node label\n",
    "3. The node has a subtree for the splitted value, in which case you can recursively continue classifying on the subtree\n",
    "\n",
    "$\\ex{4.7}$ Take a look at the `classify()` function in `DecisionTree`, which should classify a patient record consisting of an array of discrete values and an array of numeric values using the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating patient records\n",
    "\n",
    "$\\ex{4.8}$ Finally, write a `validate()` function which takes as input a trained decision tree, a validation set of patient records and corresponding labels, and returns the percentage that is classified correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(decision_tree, data_discrete, data_numeric, labels):\n",
    "    \"\"\" Classifies all patient records and compares the outcome to \n",
    "        the provided labels. Returns the percentage of elements that was classified\n",
    "        correctly.\n",
    "            data_discrete - A 2D array of ints, each row containing the discrete features for a patient.\n",
    "            data_numeric - A 2D array of floats, each row containing the numeric features for a patient.\n",
    "            labels - List of boolean labels each belonging to a patient record\"\"\"\n",
    "    # START ANSWER\n",
    "    # END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ex{4.9}$ Create a `DecisionTree` using the training data, with `tree_type` set to $1$ (currently we can only do discrete splits) and print the results when validating with the validation set created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DecisionTree called 'trained_decision_tree' with tree_type 1 using the training data\n",
    "# START ANSWER\n",
    "# END ANSWER\n",
    "\n",
    "# Verify the correctness of the validate function\n",
    "result = validate(trained_decision_tree, x_validation_disc, x_validation_num, y_validation)\n",
    "assert np.isclose(result, 0.76667)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Adding the NumericTree\n",
    "\n",
    "Now we move on to adding the numeric splits to the tree. All the code already written in the `DecisionTree` class is inherited down to the `NumericTree` class, meaning you will only need to write the two numeric-specific functions.\n",
    "The most important of these functions is the `split()` function. The numeric split is based on a split boundary, where all values smaller than the boundary are placed in one branch, and those greater or equal in the other branch.\n",
    "\n",
    "$\\ex{5.1}$ Complete the `split()` function in the `NumericTree` class below by implementing the `find_best_split` function. This function tries every possible split boundary for every feature and uses the split with the best IG overall. Think about a logical way to generate all possible ways to separate a set of numeric values into two sets using a split boundary.\n",
    "\n",
    "__Hint 1__: You do not have to iterate over *all* possible split values. You could just iterate over all values that are present in the dataset, since the values in between make no difference when splitting the data.\n",
    "\n",
    "__Hint 2__: As a last addition, you should also store the split boundary for this best split, as you will need to compare new values to this same boundary in order to classify them. You can create a new object value, like `self.boundary`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split(data, labels):\n",
    "    max_feat = None\n",
    "    max_split = None\n",
    "    max_ig = 0\n",
    "    max_boundary = None\n",
    "\n",
    "    for feat in range(data.shape[1]):\n",
    "        col = data[:, feat]\n",
    "\n",
    "        for curr_boundary in col:\n",
    "            # START ANSWER\n",
    "            # END ANSWER\n",
    "            \n",
    "    return max_feat, max_split, max_ig, max_boundary\n",
    "\n",
    "max_feat, max_split, max_ig, max_boundary = find_best_split(x_train_num[:10,:], y_train[:10])\n",
    "\n",
    "# Verify the correctness of the find_best_split function\n",
    "assert ([[*i] for i in max_split] == [[1, 3, 5, 6, 7], [0, 2, 4, 8, 9]])\n",
    "assert np.array_equal(feat_values, [0, 1, 2])\n",
    "assert max_feat == 3\n",
    "assert np.isclose(max_ig, 0.27807)  \n",
    "assert max_boundary == 159.0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericTree(DecisionTree):\n",
    "    def __init__(self, dtree):\n",
    "        \"\"\" Takes a DecisionTree as initialization parameter and copies all its\n",
    "            attributes. Then calls the split() function to determine the optimal\n",
    "            numeric variable to split this subset of the data on.\n",
    "                dtree - The DecisionTree instance whose attributes are copied to this\n",
    "                        NumericTree instance.\n",
    "            N.B. This function has already been provided and does not need to be modified.\"\"\"\n",
    "        self.__dict__ = dtree.__dict__.copy()\n",
    "        self.split()\n",
    "\n",
    "    def split(self):\n",
    "        \"\"\" Determines the best boundary for any numeric variable to split the\n",
    "            current dataset on, based on the IG resulting from the split. For this\n",
    "            best split boundary, the function stores several resulting attributes\n",
    "            from the split, using the store_split_values function. See the\n",
    "            documentation of store_split_values for an overview of what should\n",
    "            be stored. In addition, one more attribute is stored in the numeric\n",
    "            case, namely the boundary value used for the split.\"\"\"\n",
    "        max_feat, max_split, max_ig, boundary = find_best_split(self.data_numeric, self.labels)\n",
    "        self.boundary = boundary\n",
    "        \n",
    "        max_feat_values = [False, True]\n",
    "        self.store_split_values(max_feat, max_feat_values, max_split, max_ig)\n",
    "        \n",
    "    def get_subtree(self, row_discrete, row_numeric):\n",
    "        \"\"\" Returns the subtree one branch down.\n",
    "                row_discrete - array of the discrete values\n",
    "                row_numeric - array of the numeric values\"\"\"\n",
    "        value = row_numeric[self.feat_index] >= self.boundary\n",
    "        return self.branches.get(value, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ex{5.2}$ The second numeric-specific function is the `get_subtree()` function. Take a look at this function in the `NumericTree` class above, which compares the value of the variable on which the split was performed to the split boundary. It uses this to get the correct subtree from the `branches` attribute, where *False* is for the smaller values and *True* for those greater or equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparing the Results\n",
    "\n",
    "Now that the DiscreteTree and NumericTree classes are complete, we can create an actual decision tree.\n",
    "\n",
    "$\\ex{6.1}$ Create three `DecisionTree` instances using the training data, with `tree_type` set to $0$, $1$ and $2$. Tree type 1 will contain only discrete splits, tree type 2 will contain only numeric splits and tree type 0 will try both and use the split with the highest information gain. Print the average validation results for all three trees using the validation set created earlier. Repeat this process for multiple splits of the test set (for `n_iterations`).\n",
    "\n",
    "**Note**: Do not forget to pass `threshold` (to decide when to stop splitting) when creating a `DecisionTree`, because this parameter will be used in the next exercise.\n",
    "\n",
    "Keep in mind that the hybrid tree will not necessarily be the most accurate one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(threshold = 0.1, n_iterations = 50, standarization=False):\n",
    "    hybrid_accuracy = 0\n",
    "    discrete_accuracy = 0\n",
    "    numeric_accuracy = 0\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "\n",
    "        x_train_disc, x_train_num, x_validation_disc, x_validation_num, y_train, y_validation = split_dataset(x, y, i)\n",
    "        \n",
    "        # START ANSWER\n",
    "        # END ANSWER\n",
    "\n",
    "    hybrid_accuracy /= n_iterations\n",
    "    discrete_accuracy /= n_iterations\n",
    "    numeric_accuracy /= n_iterations\n",
    "    \n",
    "    return hybrid_accuracy, discrete_accuracy, numeric_accuracy\n",
    "\n",
    "hybrid, discrete, numeric = get_accuracy(0.1, standarization=False)\n",
    "\n",
    "print('Hybrid tree accuracy:', hybrid)\n",
    "print('Discrete tree accuracy:', discrete)\n",
    "print('Numeric tree accuracy:', numeric)\n",
    "\n",
    "# Verify the correctness of the get_accuracy function\n",
    "assert np.isclose(hybrid, 0.744, rtol=0.05)  \n",
    "assert np.isclose(discrete, 0.779, rtol=0.05)  \n",
    "assert np.isclose(numeric, 0.695, rtol=0.05)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ex{6.2}$ Normalize the numeric features in the dataset by using z-score normalization, and use the scaled features to train and evaluate the classifiers. What do you observe in terms of the accuracy compared to the unscaled version?  \n",
    "**Hint:** Make sure you use the same scaling for both the train and the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_standardize(threshold = 0.1, n_iterations = 50):\n",
    "    hybrid_accuracy = 0\n",
    "    discrete_accuracy = 0\n",
    "    numeric_accuracy = 0\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "    \n",
    "        x_train_disc, x_train_num, x_validation_disc, x_validation_num, y_train, y_validation = split_dataset(x, y, i)\n",
    "    \n",
    "        # START ANSWER\n",
    "        # END ANSWER\n",
    "            \n",
    "        # START ANSWER\n",
    "        # END ANSWER\n",
    "\n",
    "    hybrid_accuracy /= n_iterations\n",
    "    discrete_accuracy /= n_iterations\n",
    "    numeric_accuracy /= n_iterations\n",
    "    \n",
    "    return hybrid_accuracy, discrete_accuracy, numeric_accuracy\n",
    "\n",
    "hybrid, discrete, numeric = get_accuracy_standardize(0.1)\n",
    "\n",
    "print('Hybrid tree accuracy:', hybrid)\n",
    "print('Discrete tree accuracy:', discrete)\n",
    "print('Numeric tree accuracy:', numeric)\n",
    "\n",
    "# Verify the correctness of the get_accuracy function\n",
    "assert np.isclose(hybrid, 0.744, rtol=0.05)  \n",
    "assert np.isclose(discrete, 0.779, rtol=0.05)  \n",
    "assert np.isclose(numeric, 0.695, rtol=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to demonstrate the importance of a good threshold value for the information gain, we are going to plot the accuracy for different threshold values. Note that the code below might run for a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_values = np.linspace(0.00, 0.3, 10)\n",
    "\n",
    "accuracies_hybrid = []\n",
    "accuracies_discrete = []\n",
    "accuracies_numeric = []\n",
    "\n",
    "for threshold in threshold_values:\n",
    "    hybrid, discrete, numeric = get_accuracy(threshold, n_iterations=10)\n",
    "    accuracies_hybrid.append(hybrid)\n",
    "    accuracies_discrete.append(discrete)\n",
    "    accuracies_numeric.append(numeric)\n",
    "    \n",
    "_, axis = plt.subplots()\n",
    "axis.plot(threshold_values, accuracies_hybrid, label = 'hybrid')\n",
    "axis.plot(threshold_values, accuracies_discrete, label = 'discrete')\n",
    "axis.plot(threshold_values, accuracies_numeric, label = 'numeric')\n",
    "\n",
    "axis.legend()\n",
    "axis.set_xlabel('Threshold')\n",
    "axis.set_ylabel('Accuracy')\n",
    "plt.title('DecisionTree accuracy for different threshold values')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your algorithm is correct and you averaged over enough different validation splits, you might see some strange results in the comparison you just produced. For the last part of the assignment, answer these questions about the results.\n",
    "\n",
    "$\\q{7.1}$ Can you explain how it is possible that the validation score using just the discrete variables is higher than the validation score using the discrete and numeric variables combined? What property of the algorithm makes this outcome possible?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\q{7.2}$ What is your hypothesis for why this happens for this particular data? What could you do to improve the validation results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\q{7.3}$ Think back to the XOR example in the beginning of this assignment. Will the decision tree learning algorithm that you just implemented be able to solve this problem. Why or why not?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
